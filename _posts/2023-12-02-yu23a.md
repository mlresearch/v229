---
title: Language to Rewards for Robotic Skill Synthesis
section: Oral
openreview: SgTPdyehXMA
abstract: Large language models (LLMs) have demonstrated exciting progress in acquiring
  diverse new capabilities through in-context learning, ranging from logical reasoning
  to code-writing. Robotics researchers have also explored using LLMs to advance the
  capabilities of robotic control. However, since low-level robot actions are hardware-dependent
  and underrepresented in LLM training corpora, existing efforts in applying LLMs
  to robotics have largely treated LLMs as semantic planners or relied on human-engineered
  control primitives to interface with the robot. On the other hand, reward functions
  are shown to be flexible representations that can be optimized for control policies
  to achieve diverse tasks, while their semantic richness makes them suitable to be
  specified by LLMs. In this work, we introduce a new paradigm that harnesses this
  realization by utilizing LLMs to define reward parameters that can be optimized
  and accomplish variety of robotic tasks. Using reward as the intermediate interface
  generated by LLMs, we can effectively bridge the gap between high-level language
  instructions or corrections to low-level robot actions. Meanwhile, combining this
  with a real-time optimizer, MuJoCo MPC, empowers an interactive behavior creation
  experience where users can immediately observe the results and provide feedback
  to the system. To systematically evaluate the performance of our proposed method,
  we designed a total of 17 tasks for a simulated quadruped robot and a dexterous
  manipulator robot. We demonstrate that our proposed method reliably tackles $90%$
  of the designed tasks, while a baseline using primitive skills as the interface
  with Code-as-policies achieves $50%$ of the tasks. We further validated our method
  on a real robot arm where complex manipulation skills such as non-prehensile pushing
  emerge through our interactive system.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yu23a
month: 0
tex_title: Language to Rewards for Robotic Skill Synthesis
firstpage: 374
lastpage: 404
page: 374-404
order: 374
cycles: false
bibtex_author: Yu, Wenhao and Gileadi, Nimrod and Fu, Chuyuan and Kirmani, Sean and
  Lee, Kuang-Huei and Arenas, Montserrat Gonzalez and Chiang, Hao-Tien Lewis and Erez,
  Tom and Hasenclever, Leonard and Humplik, Jan and Ichter, Brian and Xiao, Ted and
  Xu, Peng and Zeng, Andy and Zhang, Tingnan and Heess, Nicolas and Sadigh, Dorsa
  and Tan, Jie and Tassa, Yuval and Xia, Fei
author:
- given: Wenhao
  family: Yu
- given: Nimrod
  family: Gileadi
- given: Chuyuan
  family: Fu
- given: Sean
  family: Kirmani
- given: Kuang-Huei
  family: Lee
- given: Montserrat Gonzalez
  family: Arenas
- given: Hao-Tien Lewis
  family: Chiang
- given: Tom
  family: Erez
- given: Leonard
  family: Hasenclever
- given: Jan
  family: Humplik
- given: Brian
  family: Ichter
- given: Ted
  family: Xiao
- given: Peng
  family: Xu
- given: Andy
  family: Zeng
- given: Tingnan
  family: Zhang
- given: Nicolas
  family: Heess
- given: Dorsa
  family: Sadigh
- given: Jie
  family: Tan
- given: Yuval
  family: Tassa
- given: Fei
  family: Xia
date: 2023-12-02
address:
container-title: Proceedings of The 7th Conference on Robot Learning
volume: '229'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 12
  - 2
pdf: https://proceedings.mlr.press/v229/yu23a/yu23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
