---
title: 'BM2CP: Efficient Collaborative Perception with LiDAR-Camera Modalities'
section: Poster
openreview: uJqxFjF1xWp
abstract: Collaborative perception enables agents to share complementary perceptual
  information with nearby agents. This can significantly benefit the perception performance
  and alleviate the issues of single-view perception, such as occlusion and sparsity.
  Most proposed approaches mainly focus on single modality (especially LiDAR), and
  not fully exploit the superiority of multi-modal perception. We propose an collaborative
  perception paradigm, BM2CP, which employs LiDAR and camera to achieve efficient
  multi-modal perception. BM2CP utilizes LiDAR-guided modal fusion, cooperative depth
  generation and modality-guided intermediate fusion to acquire deep interactions
  between modalities and agents. Moreover, it is capable to cope with the special
  case that one of the sensors is unavailable. Extensive experiments validate that
  it outperforms the state-of-the-art methods with 50X lower communication volumes
  in real-world autonomous driving scenarios. Our code is available at supplementary
  materials.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhao23a
month: 0
tex_title: 'BM2CP: Efficient Collaborative Perception with LiDAR-Camera Modalities'
firstpage: 1022
lastpage: 1035
page: 1022-1035
order: 1022
cycles: false
bibtex_author: Zhao, Binyu and ZHANG, Wei and Zou, Zhaonian
author:
- given: Binyu
  family: Zhao
- given: Wei
  family: ZHANG
- given: Zhaonian
  family: Zou
date: 2023-12-02
address:
container-title: Proceedings of The 7th Conference on Robot Learning
volume: '229'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 12
  - 2
pdf: https://proceedings.mlr.press/v229/zhao23a/zhao23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
