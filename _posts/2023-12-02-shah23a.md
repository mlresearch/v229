---
title: 'ViNT: A Foundation Model for Visual Navigation'
section: Oral
openreview: "-K7-1WvKO3F"
abstract: General-purpose pre-trained models (“foundation models”) have enabled practitioners
  to produce generalizable solutions for individual machine learning problems with
  datasets that are significantly smaller than those required for learning from scratch.
  Such models are typically trained on large and diverse datasets with weak supervision,
  consuming much more training data than is available for any individual downstream
  application. In this paper, we describe the Visual Navigation Transformer (ViNT),
  a foundation model that aims to bring the success of general-purpose pre-trained
  models to vision-based robotic navigation. ViNT is trained with a general goal-reaching
  objective that can be used with any navigation dataset, and employs a flexible Transformer-based
  architecture to learn navigational affordances and enable efficient adaptation to
  a variety of downstream navigational tasks. ViNT is trained on a number of existing
  navigation datasets, comprising hundreds of hours of robotic navigation from a variety
  of different robotic platforms, and exhibits positive transfer, outperforming specialist
  models trained on narrower datasets. ViNT can be augmented with diffusion-based
  goal proposals to explore novel environments, and can solve kilometer-scale navigation
  problems when equipped with long-range heuristics. ViNT can also be adapted to novel
  task specifications with a technique inspired by prompt-tuning, where the goal encoder
  is replaced by an encoding of another task modality (e.g., GPS waypoints or turn-by-turn
  directions) embedded into the same space of goal tokens. This flexibility and ability
  to accommodate a variety of downstream problem domains establish ViNT as an effective
  foundation model for mobile robotics.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shah23a
month: 0
tex_title: 'ViNT: A Foundation Model for Visual Navigation'
firstpage: 711
lastpage: 733
page: 711-733
order: 711
cycles: false
bibtex_author: Shah, Dhruv and Sridhar, Ajay and Dashora, Nitish and Stachowicz, Kyle
  and Black, Kevin and Hirose, Noriaki and Levine, Sergey
author:
- given: Dhruv
  family: Shah
- given: Ajay
  family: Sridhar
- given: Nitish
  family: Dashora
- given: Kyle
  family: Stachowicz
- given: Kevin
  family: Black
- given: Noriaki
  family: Hirose
- given: Sergey
  family: Levine
date: 2023-12-02
address:
container-title: Proceedings of The 7th Conference on Robot Learning
volume: '229'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 12
  - 2
pdf: https://proceedings.mlr.press/v229/shah23a/shah23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
