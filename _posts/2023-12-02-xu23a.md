---
title: 'XSkill: Cross Embodiment Skill Discovery'
section: Poster
openreview: 8L6pHd9aS6w
abstract: Human demonstration videos are a widely available data source for robot
  learning and an intuitive user interface for expressing desired behavior. However,
  directly extracting reusable robot manipulation skills from unstructured human videos
  is challenging due to the big embodiment difference and unobserved action parameters.
  To bridge this embodiment gap, this paper introduces XSkill, an imitation learning
  framework that 1) discovers a cross-embodiment representation called skill prototypes
  purely from unlabeled human and robot manipulation videos, 2) transfers the skill
  representation to robot actions using conditional diffusion policy, and finally,
  3) composes the learned skill to accomplish unseen tasks specified by a human prompt
  video. Our experiments in simulation and real-world environments show that the discovered
  skill prototypes facilitate both skill transfer and composition for unseen tasks,
  resulting in a more general and scalable imitation learning framework.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xu23a
month: 0
tex_title: 'XSkill: Cross Embodiment Skill Discovery'
firstpage: 3536
lastpage: 3555
page: 3536-3555
order: 3536
cycles: false
bibtex_author: Xu, Mengda and Xu, Zhenjia and Chi, Cheng and Veloso, Manuela and Song,
  Shuran
author:
- given: Mengda
  family: Xu
- given: Zhenjia
  family: Xu
- given: Cheng
  family: Chi
- given: Manuela
  family: Veloso
- given: Shuran
  family: Song
date: 2023-12-02
address:
container-title: Proceedings of The 7th Conference on Robot Learning
volume: '229'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 12
  - 2
pdf: https://proceedings.mlr.press/v229/xu23a/xu23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
