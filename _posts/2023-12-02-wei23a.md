---
title: A Bayesian Approach to Robust Inverse Reinforcement Learning
section: Poster
openreview: W5SrUCN0yUa
abstract: We consider a Bayesian approach to offline model-based inverse reinforcement
  learning (IRL). The proposed framework differs from existing offline model-based
  IRL approaches by performing simultaneous estimation of the expert’s reward function
  and subjective model of environment dynamics. We make use of a class of prior distributions
  which parameterizes how accurate the expert’s model of the environment is to develop
  efficient algorithms to estimate the expert’s reward and subjective dynamics in
  high-dimensional settings. Our analysis reveals a novel insight that the estimated
  policy exhibits robust performance when the expert is believed (a priori) to have
  a highly accurate model of the environment. We verify this observation in the MuJoCo
  environments and show that our algorithms outperform state-of-the-art offline IRL
  algorithms.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wei23a
month: 0
tex_title: A Bayesian Approach to Robust Inverse Reinforcement Learning
firstpage: 2304
lastpage: 2322
page: 2304-2322
order: 2304
cycles: false
bibtex_author: Wei, Ran and Zeng, Siliang and Li, Chenliang and Garcia, Alfredo and
  McDonald, Anthony D and Hong, Mingyi
author:
- given: Ran
  family: Wei
- given: Siliang
  family: Zeng
- given: Chenliang
  family: Li
- given: Alfredo
  family: Garcia
- given: Anthony D
  family: McDonald
- given: Mingyi
  family: Hong
date: 2023-12-02
address:
container-title: Proceedings of The 7th Conference on Robot Learning
volume: '229'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 12
  - 2
pdf: https://proceedings.mlr.press/v229/wei23a/wei23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
